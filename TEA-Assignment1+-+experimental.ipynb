{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from math import floor,log,exp\n",
    "from nltk.util import ngrams\n",
    "from nltk.metrics import distance\n",
    "from numpy import product as  prod\n",
    "from collections import Counter\n",
    "from math import log, exp\n",
    "import random\n",
    "\n",
    "# Reads file\n",
    "with open(\"europarl.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    text = file.read()\n",
    "    text = text.lower()\n",
    "\n",
    "\n",
    "# Creates train set for bigrams and trigrams\n",
    "train_set2 = sent_tokenize(text[0:floor(len(text)/50)])\n",
    "train_set3 = train_set2[:]\n",
    "\n",
    "# Creates development and test sets for bigrams and trigrams\n",
    "dev_set2 = sent_tokenize(text[89*floor(len(text)/100):90*floor(len(text)/100)])\n",
    "dev_set3 = dev_set2[:]\n",
    "\n",
    "test_set2 = sent_tokenize(text[99*floor(len(text)/100):len(text)])\n",
    "test_set3 = test_set2[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adds s1, s2, e and replaces punctuation marks\n",
    "\n",
    "datasets2 = (train_set2, dev_set2, test_set2)\n",
    "datasets3 = (train_set3, dev_set3, test_set3)\n",
    "punctuation = [\"?\",'.','!',',', '\"',\"'\", \";\", \":\", \"(\" , \")\", \"]\", \"[\", \"}\", \"{\", \"-\"]\n",
    "\n",
    "for dataset in datasets2:  \n",
    "    for i in range(len(dataset)):\n",
    "        #Checks if s1 has already been inserted from previous runs\n",
    "        if dataset[i][:4] != \"*s1*\":\n",
    "            dataset[i] = \"*s1* \" + dataset[i] + \" *e*\"\n",
    "            for mark in punctuation:\n",
    "                dataset[i] = dataset[i].replace(mark, \"\")\n",
    "\n",
    "for dataset in datasets3:                \n",
    "    for i in range(len(dataset)):\n",
    "        #Checks if s1 has already been inserted from previous runs\n",
    "        if dataset[i][:4] != \"*s1*\":\n",
    "            dataset[i] = \"*s1* *s2* \" + dataset[i] + \" *e*\"\n",
    "            for mark in punctuation:\n",
    "                dataset[i] = dataset[i].replace(mark, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates tokens for bigrams and trigrams from training set\n",
    "train_set2_tokens = [word_tokenize(i) for i in train_set2]\n",
    "train_set3_tokens = [word_tokenize(i) for i in train_set3]\n",
    "\n",
    "train_set2_tokens = [item for sublist in train_set2_tokens for item in sublist]\n",
    "train_set3_tokens = [item for sublist in train_set3_tokens for item in sublist]\n",
    "\n",
    "# Creates a dictionary that records the frequency of each word\n",
    "wordcount = Counter(train_set3_tokens)\n",
    "vocabulary = set()\n",
    "\n",
    "# Creates a set containing all frequent vocabulary words (encountered more than 10 times) and\n",
    "# replaces all infrequent words with the *UKN* token in the training set\n",
    "\n",
    "for i in range(len(train_set2_tokens)):\n",
    "    if wordcount[train_set2_tokens[i]] < 10:\n",
    "        train_set2_tokens[i] = \"*UKN*\"\n",
    "    else:\n",
    "        vocabulary.add(train_set2_tokens[i])\n",
    "        \n",
    "for i in range(len(train_set3_tokens)):\n",
    "    if wordcount[train_set3_tokens[i]] < 10:\n",
    "        train_set3_tokens[i] = \"*UKN*\"\n",
    "\n",
    "vocabulary -= {'*s1*', '*end*', '*s2*'}\n",
    "\n",
    "# Distinct words in vocabulary\n",
    "v = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = (dev_set2, dev_set3, test_set2, test_set3)\n",
    "\n",
    "# Replaces infrequent words with *UKN* in dev and test sets\n",
    "for dataset in datasets:\n",
    "    for i in range(len(dataset)):\n",
    "        for word in word_tokenize(dataset[i]):\n",
    "            if wordcount[word] < 10:\n",
    "                dataset[i] = dataset[i].replace(word, \"*UKN*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates training set bigrams and trigrams\n",
    "\n",
    "train_set_bigrams = [ gram for gram in ngrams(train_set2_tokens, 2) ]\n",
    "train_set_trigrams = [ gram for gram in ngrams(train_set3_tokens, 3) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defines the Laplace function for bigrams and trigrams\n",
    "\n",
    "def laplace_bi(bigram, pool, text):\n",
    "    return (pool.count(bigram) + 1) / (text.count(bigram[0]) + v)\n",
    "    \n",
    "def laplace_tri(trigram, pool1, pool2):\n",
    "    return (pool1.count(trigram) + 1) / (pool2.count(trigram[0:2]) + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(15)\n",
    "\n",
    "# Creates correct (encountered) and incorrect (random tokens from vocabulary) sentences, 5 words long, to test the model\n",
    "correct_sents2 = random.sample(dev_set2, 5)\n",
    "incorrect_sents2 = [random.sample(vocabulary, len(word_tokenize(i))) for i in correct_sents2]\n",
    "correct_sents3 = random.sample(dev_set3, 5)\n",
    "incorrect_sents3 = [random.sample(vocabulary, len(word_tokenize(i))) for i in correct_sents3]\n",
    "\n",
    "for i in range(len(correct_sents2)):\n",
    "    incorrect_sents2[i] = ' '.join(incorrect_sents2[i])\n",
    "    \n",
    "for i in range(len(incorrect_sents3)):\n",
    "    incorrect_sents3[i] = ' '.join(incorrect_sents2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities of correct sentences as predicted by the bigram model:\n",
      "[3.480658177108437e-31, 1.2156768873246593e-31, 2.7949436508896e-138, 3.1626842155498793e-69, 1.0703367081637254e-64]\n",
      "Probabilities of correct sentences as predicted by the trigram model:\n",
      "[3.4665263436972496e-160, 1.7484676224283213e-116, 4.971633584334035e-87, 3.377826598405568e-55, 1.0226189340867358e-208]\n",
      "Probabilities of incorrect sentences as predicted by the bigram model:\n",
      "[1.9558463657316125e-44, 4.696226811858409e-51, 4.736487320508614e-216, 6.317970371429336e-114, 5.018260386118942e-94]\n",
      "Probabilities of incorrect sentences as predicted by the trigram model:\n",
      "[2.03810927380032e-205, 8.738503696185368e-133, 9.038113808663426e-100, 2.3323321070302176e-73, 3.8142580843271875e-268]\n"
     ]
    }
   ],
   "source": [
    "# Implements the model to calculate probabilities of each sentence\n",
    "\n",
    "bi_probs_correct = []\n",
    "bi_probs_incorrect = []\n",
    "\n",
    "tri_probs_correct = []\n",
    "tri_probs_incorrect = []\n",
    "\n",
    "for i in range(len(correct_sents2)):\n",
    "    correct_sents2_tokens = word_tokenize(correct_sents2[i])\n",
    "    incorrect_sents2_tokens = ['*s1*'] + word_tokenize(incorrect_sents2[i]) + ['*e*']\n",
    "\n",
    "    correct_bigrams = [ gram for gram in ngrams(correct_sents2_tokens, 2) ]\n",
    "    incorrect_bigrams = [ gram for gram in ngrams(incorrect_sents2_tokens, 2) ]\n",
    "    \n",
    "    sum_correct = 0\n",
    "    sum_incorrect = 0\n",
    "    \n",
    "    for j in range(len(correct_bigrams)):\n",
    "        sum_correct += log(laplace_bi(correct_bigrams[j], train_set_bigrams, train_set2_tokens))\n",
    "        sum_incorrect += log(laplace_bi(incorrect_bigrams[j], train_set_bigrams, train_set2_tokens))\n",
    "        \n",
    "    bi_probs_correct.append(exp(sum_correct))\n",
    "    bi_probs_incorrect.append(exp(sum_incorrect))\n",
    "    \n",
    "for i in range(len(correct_sents2)):\n",
    "    sum_correct = 0\n",
    "    sum_incorrect = 0\n",
    "    \n",
    "    correct_sents3_tokens = word_tokenize(correct_sents3[i])\n",
    "    incorrect_sents3_tokens = ['*s1*', '*s2*'] + word_tokenize(incorrect_sents3[i]) + ['*e*']\n",
    "    \n",
    "    correct_trigrams = [ gram for gram in ngrams(correct_sents3_tokens, 3) ]\n",
    "    incorrect_trigrams = [ gram for gram in ngrams(incorrect_sents3_tokens, 3) ] + ['*e*']\n",
    "    \n",
    "    for k in range(len(correct_trigrams)):\n",
    "        sum_correct += log(laplace_tri(correct_trigrams[k], train_set_trigrams, train_set3_tokens))\n",
    "        sum_incorrect += log(laplace_tri(incorrect_trigrams[k], train_set_trigrams, train_set3_tokens))\n",
    "        \n",
    "    tri_probs_correct.append(exp(sum_correct))\n",
    "    tri_probs_incorrect.append(exp(sum_incorrect))\n",
    "    \n",
    "print(\"Probabilities of correct sentences as predicted by the bigram model:\")    \n",
    "print(bi_probs_correct)\n",
    "print(\"Probabilities of correct sentences as predicted by the trigram model:\")  \n",
    "print(tri_probs_correct)\n",
    "print(\"Probabilities of incorrect sentences as predicted by the bigram model:\")   \n",
    "print(bi_probs_incorrect)\n",
    "print(\"Probabilities of incorrect sentences as predicted by the trigram model:\") \n",
    "print(tri_probs_incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'to' with a probability of 0.03319108582266477\n"
     ]
    }
   ],
   "source": [
    "# Implements a prediction for a given word sequence. Returns the predicted word and its probability\n",
    "# Bigram model\n",
    "\n",
    "trial_sent = 'you want'\n",
    "trial_token = word_tokenize(trial_sent)\n",
    " \n",
    "max_bi = 0\n",
    "for word in vocabulary:\n",
    "    trial_bi = (trial_token[-1], word)\n",
    "    prob = laplace_bi(trial_bi, train_set_bigrams, train_set2_tokens)\n",
    "    if prob > max_bi:\n",
    "        max_bi = prob\n",
    "        best_choice_bi = word\n",
    "\n",
    "print(\"'\" + best_choice_bi + \"' with a probability of \" + str(max_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'to' with a probability of 0.002997002997002997\n"
     ]
    }
   ],
   "source": [
    "# Trigram Model\n",
    "\n",
    "max_tri = 0\n",
    "for word in vocabulary:\n",
    "    trial_tri = (trial_token[-2], trial_token[-1], word)\n",
    "    prob = laplace_tri(trial_tri, train_set_trigrams, train_set3_tokens)\n",
    "    if prob > max_tri:\n",
    "        max_tri = prob\n",
    "        best_choice_tri = word\n",
    "\n",
    "print(\"'\" + best_choice_tri + \"' with a probability of \" + str(max_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculates the cross entropy and perplexity of the bigram model for the whole development set\n",
    "\n",
    "probabilities = []\n",
    "\n",
    "for sentence in dev_set2:\n",
    "    dev_set_tokens_bi = word_tokenize(sentence)\n",
    "    dev_set_bigrams = [ gram for gram in ngrams(dev_set_tokens_bi, 2) ]\n",
    "    for bigram in dev_set_bigrams:\n",
    "        probabilities.append(laplace_bi(bigram, train_set_bigrams, train_set2_tokens))\n",
    "        \n",
    "sum1 = 0\n",
    "\n",
    "for probability in probabilities:\n",
    "    sum1 += log(probability, 2)\n",
    "\n",
    "cross_entropy_bigram = (-1 / len(probabilities)) * sum1\n",
    "perplexity_bigram = 2**cross_entropy_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculates the cross entropy and perplexity of the trigram model for the whole development set\n",
    "\n",
    "probabilities_tri = []\n",
    "\n",
    "for sentence in dev_set3:\n",
    "    dev_set_tokens_tri = word_tokenize(sentence)\n",
    "    dev_set_trigrams = [ gram for gram in ngrams(dev_set_tokens_tri, 2) ]\n",
    "    for trigram in dev_set_trigrams:\n",
    "        probabilities_tri.append(laplace_tri(trigram, train_set_trigrams, train_set3_tokens))\n",
    "        \n",
    "sum1_tri = 0\n",
    "\n",
    "for probability in probabilities_tri:\n",
    "    sum1_tri += log(probability, 2)\n",
    "\n",
    "cross_entropy_trigram = (-1 / len(probabilities_tri)) * sum1_tri\n",
    "perplexity_trigram = 2**cross_entropy_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy of the bigram model: 7.636271324206641\n",
      "Perplexity of the bigram model: 198.95127358084127\n",
      "Cross entropy of the trigram model: 10.967226258838277\n",
      "Perplexity of the trigram model: 2002.000000003169\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross entropy of the bigram model: {}\".format(cross_entropy_bigram))\n",
    "print(\"Perplexity of the bigram model: {}\".format(perplexity_bigram))\n",
    "\n",
    "print(\"Cross entropy of the trigram model: {}\".format(cross_entropy_trigram))\n",
    "print(\"Perplexity of the trigram model: {}\".format(perplexity_trigram))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
